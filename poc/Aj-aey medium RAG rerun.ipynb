{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is for blind implement RAG concept on medium blog by AJ Aey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook is duplicated from Aj-aey medium RAG. it is rerun after conda env change.\n",
    "conda env change from kdai-llm-final -> kdai-llm-finalfix.\n",
    "this new env is now installed on built-in storage.\n",
    "pdf corpus storage also moved from external storage to built-in storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.11.17-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.17 (from llama-index)\n",
      "  Using cached llama_index_core-0.11.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.2.13-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.2)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.10.5)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Downloading networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (10.4.0)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.11.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
      "  Using cached llama_cloud-0.1.2-py3-none-any.whl.metadata (763 bytes)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
      "  Using cached llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (0.14.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading jiter-0.6.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (2.2.3)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.11.17-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.11.17-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "Downloading llama_index_llms_openai-0.2.13-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.1.2-py3-none-any.whl (173 kB)\n",
      "Using cached llama_parse-0.5.7-py3-none-any.whl (10 kB)\n",
      "Downloading networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (982 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl (274 kB)\n",
      "Downloading jiter-0.6.1-cp312-cp312-macosx_11_0_arm64.whl (300 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, wrapt, SQLAlchemy, pypdf, pydantic-core, networkx, mypy-extensions, marshmallow, joblib, jiter, greenlet, distro, annotated-types, typing-inspect, tiktoken, pydantic, nltk, deprecated, openai, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed SQLAlchemy-2.0.35 annotated-types-0.7.0 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 greenlet-3.1.1 jiter-0.6.1 joblib-1.4.2 llama-cloud-0.1.2 llama-index-0.11.17 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.17 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.13 llama-index-multi-modal-llms-openai-0.2.2 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.7 marshmallow-3.22.0 mypy-extensions-1.0.0 networkx-3.4.1 nltk-3.9.1 openai-1.51.2 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-4.3.1 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-readers-elasticsearch\n",
      "  Using cached llama_index_readers_elasticsearch-0.2.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-readers-elasticsearch) (0.11.17)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.4.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.11.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (3.22.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-elasticsearch) (24.1)\n",
      "Using cached llama_index_readers_elasticsearch-0.2.1-py3-none-any.whl (3.3 kB)\n",
      "Installing collected packages: llama-index-readers-elasticsearch\n",
      "Successfully installed llama-index-readers-elasticsearch-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-readers-elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-vector-stores-opensearch\n",
      "  Using cached llama_index_vector_stores_opensearch-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-vector-stores-opensearch) (0.11.17)\n",
      "Collecting opensearch-py<3.0.0,>=2.4.2 (from opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch)\n",
      "  Using cached opensearch_py-2.7.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.4.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from opensearch-py<3.0.0,>=2.4.2->opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from opensearch-py<3.0.0,>=2.4.2->opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch) (2024.8.30)\n",
      "Collecting Events (from opensearch-py<3.0.0,>=2.4.2->opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch)\n",
      "  Using cached Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from opensearch-py<3.0.0,>=2.4.2->opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch) (2.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.11.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (3.22.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from python-dateutil->opensearch-py<3.0.0,>=2.4.2->opensearch-py[async]<3.0.0,>=2.4.2->llama-index-vector-stores-opensearch) (1.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-opensearch) (24.1)\n",
      "Using cached llama_index_vector_stores_opensearch-0.3.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached opensearch_py-2.7.1-py3-none-any.whl (325 kB)\n",
      "Using cached Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: Events, opensearch-py, llama-index-vector-stores-opensearch\n",
      "Successfully installed Events-0.5 llama-index-vector-stores-opensearch-0.3.0 opensearch-py-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-vector-stores-opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-ollama\n",
      "  Using cached llama_index_embeddings_ollama-0.3.1-py3-none-any.whl.metadata (693 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-embeddings-ollama) (0.11.17)\n",
      "Collecting ollama<0.4.0,>=0.3.1 (from llama-index-embeddings-ollama)\n",
      "  Using cached ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.4.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.11.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (3.22.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-ollama) (24.1)\n",
      "Using cached llama_index_embeddings_ollama-0.3.1-py3-none-any.whl (2.6 kB)\n",
      "Using cached ollama-0.3.3-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: ollama, llama-index-embeddings-ollama\n",
      "Successfully installed llama-index-embeddings-ollama-0.3.1 ollama-0.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-huggingface\n",
      "  Using cached llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.6)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-embeddings-huggingface) (0.11.17)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.5)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Using cached minijinja-2.2.0-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.0.35)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.4.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.44.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.11.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.1.1)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (75.1.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.22.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "Using cached minijinja-2.2.0-cp38-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (1.6 MB)\n",
      "Using cached torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, minijinja, torch, scikit-learn, sentence-transformers, llama-index-embeddings-huggingface\n",
      "Successfully installed llama-index-embeddings-huggingface-0.3.1 minijinja-2.2.0 mpmath-1.3.0 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.2.0 sympy-1.13.3 threadpoolctl-3.5.0 torch-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now preparation section has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kdai-llm-finalfix/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import torch\n",
    "import nest_asyncio\n",
    "from os import getenv\n",
    "\n",
    "# Apply nest_asyncio to avoid runtime errors in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Check if CUDA is available for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now setup hybrid search pipeline on opensearch.\n",
    "to run this code, ensure opensearch container is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}"
     ]
    }
   ],
   "source": [
    "! curl -XPUT \"http://localhost:9200/_search/pipeline/hybrid-search-pipeline\" -H 'Content-Type: application/json' -d' \\\n",
    "{ \\\n",
    "  \"description\": \"Pipeline for hybrid search\", \\\n",
    "  \"phase_results_processors\": [ \\\n",
    "    { \\\n",
    "      \"normalization-processor\": { \\\n",
    "        \"normalization\": { \\\n",
    "          \"technique\": \"min_max\" \\\n",
    "        }, \\\n",
    "        \"combination\": { \\\n",
    "          \"technique\": \"harmonic_mean\", \\\n",
    "          \"parameters\": { \\\n",
    "            \"weights\": [ \\\n",
    "              0.3, \\\n",
    "              0.7 \\\n",
    "            ] \\\n",
    "          } \\\n",
    "        } \\\n",
    "      } \\\n",
    "    } \\\n",
    "  ] \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load and process PDF documents\n",
    "In this step, we’ll load PDF documents and convert them into a format suitable for further processing, using Llama Index to help manage the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Load PDF documents:\n",
    "Explanation: We use SimpleDirectoryReader to load PDF files from a specified directory. This function is part of Llama Index and is designed to handle various document formats, including PDFs. It automatically extracts text content from the PDFs, making it easier to process the information in subsequent steps.\n",
    "\n",
    "PDF file containing the article 291 judgement will be placed in this folder.\n",
    "it will not be part of opensearch container. it will also not sync to github.\n",
    "this data source will be compressed and backup on onedrive instead of github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:38:47.976424\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the pdf corpus\n",
    "#external_disk_path = \"/Volumes/SS2305_1MH01/data-src/kdai-llm-final-20241007/pdf_corpus\"  # macOS/Linux\n",
    "pdf_corpus_disk_path = \"/Users/ekkalukw/data-src/kdai-llm-final-20241007/pdf_corpus\"  # macOS/Linux\n",
    "# external_disk_path = \"D:/pdf_corpus\"  # Windows\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:40:48.650927\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "#reader = SimpleDirectoryReader(input_dir=\"pdf_corpus\",recursive=True)\n",
    "reader = SimpleDirectoryReader(input_dir=pdf_corpus_disk_path,recursive=True)\n",
    "documents = reader.load_data()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Split documents into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 88/88 [00:00<00:00, 366.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:41:17.266717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    separator=\" \",\n",
    ")\n",
    "token_nodes = splitter.get_nodes_from_documents(\n",
    "    documents, show_progress=True\n",
    ")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Generate embeddings using the BAAI/bge-m3 model\n",
    "In this step, we’ll use the BAAI/bge-m3 model to generate embeddings for our text chunks. These embeddings are crucial for semantic search and understanding the content of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:41:41.697602\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embedding_model_name = 'BAAI/bge-m3'\n",
    "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name,max_length=512, device=device)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension of example text ===> 1024\n",
      "2024-10-13 15:42:11.759695\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_model.get_text_embedding(\"box\")\n",
    "dim = len(embeddings)\n",
    "print(\"embedding dimension of example text ===>\",dim)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Setup and Initialize OpenSearch Vector Client\n",
    "In this step, we’ll set up and initialize the OpenSearch Vector Client, preparing our system to use OpenSearch as a vector database for efficient similarity searches in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Set up OpenSearch connection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:42:49.814608\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "from llama_index.vector_stores.opensearch import (\n",
    "    OpensearchVectorStore,\n",
    "    OpensearchVectorClient,\n",
    ")\n",
    "\n",
    "# http endpoint for your cluster (opensearch required for vector index usage)\n",
    "endpoint = getenv(\"OPENSEARCH_ENDPOINT\", \"http://localhost:9200\")\n",
    "# index to demonstrate the VectorStore impl\n",
    "idx = getenv(\"OPENSEARCH_INDEX\", \"test_pdf_index\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Configure OpenSearchVectorClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:43:08.537316\n"
     ]
    }
   ],
   "source": [
    "# OpensearchVectorClient stores text in this field by default\n",
    "text_field = \"content_text\"\n",
    "# OpensearchVectorClient stores embeddings in this field by default\n",
    "embedding_field = \"embedding\"\n",
    "# OpensearchVectorClient encapsulates logic for a\n",
    "# single opensearch index with vector search enabled with hybrid search pipeline\n",
    "client = OpensearchVectorClient(\n",
    "    endpoint=endpoint,\n",
    "    index=idx,\n",
    "    dim=dim,\n",
    "    embedding_field=embedding_field,\n",
    "    text_field=text_field,\n",
    "    search_pipeline=\"hybrid-search-pipeline\",\n",
    ")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Initialize OpensearchVectorStore\n",
    "We create an instance of OpensearchVectorStore using the configured client. This provides an interface for managing the vector store within the Llama Index framework, facilitating easy integration with other Llama Index functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:43:23.297579\n"
     ]
    }
   ],
   "source": [
    "# initialize vector store\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create VectorStoreIndex and Store Embeddings\n",
    "In this step, we’ll create a VectorStoreIndex using Llama Index and explicitly store our document embeddings in OpenSearch. This process enables efficient semantic search and retrieval of information from our processed documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Create a StorageContext\n",
    "Here, we create a StorageContext using the vector_store (OpenSearch) we initialized in the previous step. This StorageContext provides a standardized interface for managing storage in Llama Index and connects our index to the OpenSearch vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:43:32.716816\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Create VectorStoreIndex, Generate and Store Embeddings\n",
    "In this crucial step, we create the VectorStoreIndex, generate embeddings for our documents, and store them in OpenSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:50:09.607841\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "    token_nodes, storage_context=storage_context, embed_model=embedding_model\n",
    ")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use the index to answer questions about the PDF content\n",
    "In this section, we’ll use our created index to answer questions about the content of our PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Set up the retriever\n",
    "We create a retriever from our index with the following parameters:\n",
    "\n",
    "similarity_top_k=3: This specifies that we want to retrieve the top 3 most similar results.\n",
    "\n",
    "vector_store_query_mode=VectorStoreQueryMode.HYBRID: This enables hybrid search, combining both keyword and semantic search for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:50:35.470711\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "retriever = index.as_retriever(similarity_top_k=3,vector_store_query_mode=VectorStoreQueryMode.HYBRID)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Define the query\n",
    "This is the question we want to ask about our PDF content.\n",
    "\n",
    "this part we should use our own question. it is the question regarding legal principle from article 291 criminal law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question is deliberately not mention to article 291 in criminal law. we wan to see whether the model can understand this and retrive the correct document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:50:50.790740\n"
     ]
    }
   ],
   "source": [
    "#question = \"นายดำเข้าแย่งมีดจากนายแดง และนายดำถูกแทงเสียชีวิต หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย นายแดงจะอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้หรือไม่ ศาลจะวินิจฉัยอย่างไร\"\n",
    "question = \"นายดำเข้าแย่งมีดจากนายแดง และนายดำถูกแทงเสียชีวิต หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย นายแดงจะอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้หรือไม่ เพราะอะไร\"\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"นายดำเข้าแย่งมีดจากนายแดง และนายดำถูกแทงเสียชีวิต หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย นายแดงจะอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้หรือไม่ เพราะอะไร ให้ยกตัวอย่างคำพิพากษาศาลฎีกาหรือหมายเลขคดีแดงเรื่องนี้ด้วย\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Retrieve relevant information\n",
    "We use the retriever to find the most relevant information from our indexed documents based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:51:11.783539\n"
     ]
    }
   ],
   "source": [
    "prompt = retriever.retrieve(question)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = retriever.retrieve(question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4 Display the results\n",
    "We iterate through the retrieved results, printing both the metadata and the content of each retrieved chunk.\n",
    "\n",
    "This step demonstrates how we can use our indexed documents to retrieve relevant information based on a natural language query. The hybrid search mode allows us to leverage both keyword matching and semantic similarity, potentially providing more accurate and contextually relevant results.\n",
    "\n",
    "By using this approach, we can efficiently answer questions about the content of our PDF documents, even if the exact wording doesn’t match what’s in the documents. This is particularly useful for creating intelligent document querying systems or chatbots that can understand and respond to questions about specific document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '5', 'file_name': '1597_2562.pdf', 'file_path': '/Users/ekkalukw/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-13', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: cc7cbd18-fd52-4510-afb5-dd105c48e26d\n",
      "Text: 68 \u0000องเ\u0000นการกระ\u0000โดยเจตนา  เ\u0000อ\u0000อเ\u0000จจ\u0000ง\u0000งไ\u0000\u0000า\u0000เลยเอาอา\u0000ธ\u0000นออก\n",
      "มา\u0000ง\u0000\u0000ตายใน\u0000ดแรก  และเ\u0000อกอดป\u0000\u0000น  กระ\u0000น\u0000น\u0000ก\u0000ตาย  2 \u0000ด  เ\u0000นการ\u0000\u0000น\u0000น\n",
      "โดยประมาทเ\u0000นเห\u0000ใ\u0000\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000กระ\u0000โดยเจตนา  การกระ\u0000ของ\n",
      "\u0000เลย\u0000งไ\u0000ใ\u0000เ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตามประมวลกฎหมายอาญา  มาตรา 68\n",
      "\u0000กา\u0000อ\u0000ของ\u0000เลย\u0000งไ\u0000\u0000น \u0000วน\u0000\u0000เลย\u0000กาขอใ\u0000รอทางลงโทษ\u0000น  เ\u0000น\u0000า\n",
      "พฤ\u0000การ\u0000\u0000\u0000เลยพาอา\u0000ธ\u0000นไป \u0000ง\u0000าน\u0000เ\u0000ดเห\u0000โดย...\n",
      "Score:  1.000\n",
      "\n",
      "{'page_label': '1', 'file_name': '1597_2562.pdf', 'file_path': '/Volumes/SS2305_1MH01/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-08', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: b4d6695e-9edd-4456-96a5-f0423e75d7fe\n",
      "Text: \u0000กา\u0000ด\u0000นเ\u0000ยว\u0000บ\u0000ญหา\u0000อกฎหมาย \u0000\u0000พากษาศาล\u0000กา\u0000\n",
      "1597/2562พ\u0000กงาน\u0000ยการ\u0000งห\u0000ด\u0000มพร โจท\u0000 นาย ส . \u0000เลย ป.อ. มาตรา  68, 288,\n",
      "291 การกระ\u0000\u0000งจะเ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตาม  ป . อ . มาตรา  68 \u0000อง\n",
      "เ\u0000นการกระ\u0000โดยเจตนา  \u0000เลยเอาอา\u0000ธ\u0000นออกมา\u0000\u0000ตายและ\u0000\u0000น\u0000นโดยประมาท\n",
      "\u0000ก\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000การกระ\u0000โดยเจตนา  การกระ\u0000ของ\u0000เลย\u0000งไ\u0000ใ\u0000การ \u0000อง\u0000น\n",
      "___________________________ โจท\u0000\u0000องขอใ\u0000...\n",
      "Score:  1.000\n",
      "\n",
      "{'page_label': '1', 'file_name': '1597_2562.pdf', 'file_path': '/Users/ekkalukw/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-13', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: ef3d95fd-6094-4252-95cf-c79bae87224e\n",
      "Text: \u0000กา\u0000ด\u0000นเ\u0000ยว\u0000บ\u0000ญหา\u0000อกฎหมาย \u0000\u0000พากษาศาล\u0000กา\u0000\n",
      "1597/2562พ\u0000กงาน\u0000ยการ\u0000งห\u0000ด\u0000มพร โจท\u0000 นาย ส . \u0000เลย ป.อ. มาตรา  68, 288,\n",
      "291 การกระ\u0000\u0000งจะเ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตาม  ป . อ . มาตรา  68 \u0000อง\n",
      "เ\u0000นการกระ\u0000โดยเจตนา  \u0000เลยเอาอา\u0000ธ\u0000นออกมา\u0000\u0000ตายและ\u0000\u0000น\u0000นโดยประมาท\n",
      "\u0000ก\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000การกระ\u0000โดยเจตนา  การกระ\u0000ของ\u0000เลย\u0000งไ\u0000ใ\u0000การ \u0000อง\u0000น\n",
      "___________________________ โจท\u0000\u0000องขอใ\u0000...\n",
      "Score:  1.000\n",
      "\n",
      "2024-10-13 15:51:28.180544\n"
     ]
    }
   ],
   "source": [
    "for r in prompt:\n",
    "    print(r.metadata)\n",
    "    print(r)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '1', 'file_name': '1597_2562.pdf', 'file_path': '/Volumes/SS2305_1MH01/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-08', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: b4d6695e-9edd-4456-96a5-f0423e75d7fe\n",
      "Text: \u0000กา\u0000ด\u0000นเ\u0000ยว\u0000บ\u0000ญหา\u0000อกฎหมาย \u0000\u0000พากษาศาล\u0000กา\u0000\n",
      "1597/2562พ\u0000กงาน\u0000ยการ\u0000งห\u0000ด\u0000มพร โจท\u0000 นาย ส . \u0000เลย ป.อ. มาตรา  68, 288,\n",
      "291 การกระ\u0000\u0000งจะเ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตาม  ป . อ . มาตรา  68 \u0000อง\n",
      "เ\u0000นการกระ\u0000โดยเจตนา  \u0000เลยเอาอา\u0000ธ\u0000นออกมา\u0000\u0000ตายและ\u0000\u0000น\u0000นโดยประมาท\n",
      "\u0000ก\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000การกระ\u0000โดยเจตนา  การกระ\u0000ของ\u0000เลย\u0000งไ\u0000ใ\u0000การ \u0000อง\u0000น\n",
      "___________________________ โจท\u0000\u0000องขอใ\u0000...\n",
      "Score:  1.000\n",
      "\n",
      "{'page_label': '5', 'file_name': '1597_2562.pdf', 'file_path': '/Volumes/SS2305_1MH01/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-08', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: b1a95e97-af8f-4d57-a2c2-891640e53402\n",
      "Text: 68 \u0000องเ\u0000นการกระ\u0000โดยเจตนา  เ\u0000อ\u0000อเ\u0000จจ\u0000ง\u0000งไ\u0000\u0000า\u0000เลยเอาอา\u0000ธ\u0000นออก\n",
      "มา\u0000ง\u0000\u0000ตายใน\u0000ดแรก  และเ\u0000อกอดป\u0000\u0000น  กระ\u0000น\u0000น\u0000ก\u0000ตาย  2 \u0000ด  เ\u0000นการ\u0000\u0000น\u0000น\n",
      "โดยประมาทเ\u0000นเห\u0000ใ\u0000\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000กระ\u0000โดยเจตนา  การกระ\u0000ของ\n",
      "\u0000เลย\u0000งไ\u0000ใ\u0000เ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตามประมวลกฎหมายอาญา  มาตรา 68\n",
      "\u0000กา\u0000อ\u0000ของ\u0000เลย\u0000งไ\u0000\u0000น \u0000วน\u0000\u0000เลย\u0000กาขอใ\u0000รอทางลงโทษ\u0000น  เ\u0000น\u0000า\n",
      "พฤ\u0000การ\u0000\u0000\u0000เลยพาอา\u0000ธ\u0000นไป \u0000ง\u0000าน\u0000เ\u0000ดเห\u0000โดย...\n",
      "Score:  1.000\n",
      "\n",
      "{'page_label': '5', 'file_name': '1597_2562.pdf', 'file_path': '/Volumes/SS2305_1MH01/data-src/kdai-llm-final-20241007/pdf_corpus/1597_2562.pdf', 'file_type': 'application/pdf', 'file_size': 124005, 'creation_date': '2024-10-07', 'last_modified_date': '2024-10-03'}\n",
      "Node ID: 901377d8-8506-4ce3-aba8-671bfa5bbb73\n",
      "Text: 68 \u0000องเ\u0000นการกระ\u0000โดยเจตนา  เ\u0000อ\u0000อเ\u0000จจ\u0000ง\u0000งไ\u0000\u0000า\u0000เลยเอาอา\u0000ธ\u0000นออก\n",
      "มา\u0000ง\u0000\u0000ตายใน\u0000ดแรก  และเ\u0000อกอดป\u0000\u0000น  กระ\u0000น\u0000น\u0000ก\u0000ตาย  2 \u0000ด  เ\u0000นการ\u0000\u0000น\u0000น\n",
      "โดยประมาทเ\u0000นเห\u0000ใ\u0000\u0000ตาย\u0000งแ\u0000ความตายไ\u0000ใ\u0000กระ\u0000โดยเจตนา  การกระ\u0000ของ\n",
      "\u0000เลย\u0000งไ\u0000ใ\u0000เ\u0000นการ\u0000อง\u0000นโดยชอบ\u0000วยกฎหมายตามประมวลกฎหมายอาญา  มาตรา 68\n",
      "\u0000กา\u0000อ\u0000ของ\u0000เลย\u0000งไ\u0000\u0000น \u0000วน\u0000\u0000เลย\u0000กาขอใ\u0000รอทางลงโทษ\u0000น  เ\u0000น\u0000า\n",
      "พฤ\u0000การ\u0000\u0000\u0000เลยพาอา\u0000ธ\u0000นไป \u0000ง\u0000าน\u0000เ\u0000ดเห\u0000โดย...\n",
      "Score:  0.003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in prompt2:\n",
    "    print(r.metadata)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Implement Few-Shot Learning with OpenThaiGPT on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1 Set up Ollama with OpenThaiGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: unknown command \"search\" for \"ollama\"\n"
     ]
    }
   ],
   "source": [
    "!ollama search openthaigpt\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \u001b[?25h\n",
      "Error: pull model manifest: file does not exist\n"
     ]
    }
   ],
   "source": [
    "!ollama pull openthaigpt:latest\n",
    "print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED   \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    5 days ago    \n",
      "2024-10-13 21:00:32.561727\n"
     ]
    }
   ],
   "source": [
    "! ollama list\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2 Create a function to query OpenThaiGPT\n",
    "This function takes the user’s question and the retrieved context, formats them into a prompt, and sends it to the OpenThaiGPT model running on Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#this is example from Aj. we skip this part and use our own prompt regarding article 291 in the next cell.\\nimport ollama\\n\\ndef query_openthaigpt(question, context):\\n    formatted_prompt = f\\'\\'\\'# Few-shot examples\\nExample 1:\\nContext: An ONT (Optical Network Terminal) device is unable to connect to the network.\\nQuestion: What are the steps to troubleshoot an ONT that cannot connect?\\nAnswer: 1. Check the fiber optic cable connection.\\n2. Restart the ONT device.\\n3. Verify the status lights on the device.\\n4. Test the connection with a spare ONT.\\n5. Contact technical support if the issue persists.\\n\\nExample 2:\\nContext: A customer complains about slow internet speed in their FTTX system.\\nQuestion: What is the procedure for diagnosing and fixing slow internet speed in an FTTX system?\\nAnswer: 1. Run an internet speed test using a speed testing tool.\\n2. Check the Wi-Fi router settings.\\n3. Test the connection directly to the ONT using an Ethernet cable.\\n4. Verify the optical signal strength at the ONT.\\n5. Check the customer\\'s bandwidth usage.\\n6. Adjust device settings or replace faulty equipment if necessary.\\n\\n# RAG component\\nRetrieved information:\\n{context}\\n\\n# Actual question\\nQuestion: {question}\\n\\nPlease answer the question with Thai language using the information from the examples and the retrieved information above. Focus on troubleshooting end-user devices in FTTX projects. Provide a clear, step-by-step answer, prioritized in order of importance. Include a brief explanation for each step. If the provided information is insufficient to answer the question completely, state \"The available information is not sufficient to fully answer this question\" and suggest general troubleshooting steps.\\n\\nAnswer:\\'\\'\\'\\n\\n    #print(formatted_prompt)\\n    response = ollama.generate(model=\\'openthaigpt:latest\\', prompt=formatted_prompt)\\n    return response[\\'response\\']\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#this is example from Aj. we skip this part and use our own prompt regarding article 291 in the next cell.\n",
    "import ollama\n",
    "\n",
    "def query_openthaigpt(question, context):\n",
    "    formatted_prompt = f'''# Few-shot examples\n",
    "Example 1:\n",
    "Context: An ONT (Optical Network Terminal) device is unable to connect to the network.\n",
    "Question: What are the steps to troubleshoot an ONT that cannot connect?\n",
    "Answer: 1. Check the fiber optic cable connection.\n",
    "2. Restart the ONT device.\n",
    "3. Verify the status lights on the device.\n",
    "4. Test the connection with a spare ONT.\n",
    "5. Contact technical support if the issue persists.\n",
    "\n",
    "Example 2:\n",
    "Context: A customer complains about slow internet speed in their FTTX system.\n",
    "Question: What is the procedure for diagnosing and fixing slow internet speed in an FTTX system?\n",
    "Answer: 1. Run an internet speed test using a speed testing tool.\n",
    "2. Check the Wi-Fi router settings.\n",
    "3. Test the connection directly to the ONT using an Ethernet cable.\n",
    "4. Verify the optical signal strength at the ONT.\n",
    "5. Check the customer's bandwidth usage.\n",
    "6. Adjust device settings or replace faulty equipment if necessary.\n",
    "\n",
    "# RAG component\n",
    "Retrieved information:\n",
    "{context}\n",
    "\n",
    "# Actual question\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question with Thai language using the information from the examples and the retrieved information above. Focus on troubleshooting end-user devices in FTTX projects. Provide a clear, step-by-step answer, prioritized in order of importance. Include a brief explanation for each step. If the provided information is insufficient to answer the question completely, state \"The available information is not sufficient to fully answer this question\" and suggest general troubleshooting steps.\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "    #print(formatted_prompt)\n",
    "    response = ollama.generate(model='openthaigpt:latest', prompt=formatted_prompt)\n",
    "    return response['response']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถาม: นาย A ฆ่าคนตายโดยอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม ป.อาญา ม.68 หากข้อเท็จจริงฟังได้ว่ามีการยื้อแย่งอาวุธปืนและอาวุธปืนลั่น ศาลจะวินิจฉัยอย่างไร\n",
    "\n",
    "ตอบ: ศาลจะตดสินว่าคำให้การของจำเลยฟังไม่ขึ้น ด้วยว่าการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม ม.68 จะต้องเป็นการกระทำโดยเจตนา เมื่อข้อเท็จจริงฟังได้ว่ามีการยื้อแย่งปืน และปืนลั่น จึงเป็นการกระทำโดยประมาท เป็นเหตุให้ถึงแก่ความตาย ไม่ใช่การกระทำโดยเจตนา ไม่เป็นการป้องกันตัวโดยชอบด้วยกฎหมาย\n",
    "\n",
    "#english text translated by Claude AI 3.5 sonnet.\n",
    "Question: Mr. A killed a person and claimed it was justifiable self-defense under Section 68 of the Criminal Code. If the facts indicate that there was a struggle for a firearm and the gun accidentally discharged, how would the court rule?\n",
    "Answer: The court would likely determine that the defendant's claim of self-defense is not tenable. Justifiable self-defense under Section 68 requires intentional action. Given that the facts indicate a struggle for the firearm and an accidental discharge, this constitutes a negligent act resulting in death, not an intentional act. Therefore, it does not meet the criteria for justifiable self-defense under the law.\n",
    "The court would likely rule that this case falls under negligent homicide rather than justifiable self-defense, as the element of intent, which is crucial for the self-defense claim, is absent in the scenario where the firearm discharged accidentally during a struggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 21:03:31.224248\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def query_openthaigpt(question, context):\n",
    "    formatted_prompt = f'''# Few-shot examples\n",
    "Example 1:\n",
    "Context: Mr. A killed a person and claimed it was justifiable self-defense under Section 68 of the Criminal Code.\n",
    "If the facts indicate that there was a struggle for a firearm and the gun accidentally discharged.\n",
    "Question: How would the court rule? Please provide an example Supreme Court decision case number or the Red case number (หมายเลขคดีแดง)\n",
    "Answer: The court would likely determine that the defendant's claim of self-defense is not tenable. Justifiable self-defense under Section 68 requires intentional action. Given that the facts indicate a struggle for the firearm and an accidental discharge, this constitutes a negligent act resulting in death, not an intentional act. Therefore, it does not meet the criteria for justifiable self-defense under the law.\n",
    "The court would likely rule that this case falls under negligent homicide rather than justifiable self-defense,\n",
    "as the element of intent, which is crucial for the self-defense claim,\n",
    "is absent in the scenario where the firearm discharged accidentally during a struggle.\n",
    "The example Supreme Court decision is Case No. 1597/2562, Red Case No. อ632/2560.\n",
    "\n",
    "# RAG component\n",
    "Retrieved information:\n",
    "{context}\n",
    "\n",
    "# Actual question\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question with Thai language using the information from the examples and the retrieved information above. Focus on troubleshooting end-user devices in FTTX projects. Provide a clear, step-by-step answer, prioritized in order of importance. Include a brief explanation for each step. If the provided information is insufficient to answer the question completely, state \"The available information is not sufficient to fully answer this question\" and suggest general troubleshooting steps.\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "    #print(formatted_prompt)\n",
    "    #response = ollama.generate(model='openthaigpt:latest', prompt=formatted_prompt)\n",
    "    response = ollama.generate(model='llama3.2:latest', prompt=formatted_prompt)\n",
    "    return response['response']\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3 Integrate OpenThaiGPT with our retriever\n",
    "Now, let’s modify our question-answering process to use OpenThaiGPT:\n",
    "\n",
    "This function retrieves relevant information using our previously set up retriever, combines the retrieved text into a context, and then uses OpenThaiGPT to generate an answer based on this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: นายดำเข้าแย่งมีดจากนายแดง และนายดำถูกแทงเสียชีวิต หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย นายแดงจะอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้หรือไม่ เพราะอะไร\n",
      "Answer: คำตอบ: ไม่ได้ตามมาตรา 68 ของประมวลกฎหมายอาญา เพราะ การกระทำของนายแดงในการใช้มีดเพื่อโจมตีนายดำไม่ได้เป็นการป้องกันตัวโดยชอบด้วยกฎหมาย แต่เป็นการละเมิดสิทธิของผู้อื่น และไม่มีการใช้กำลังโดยเจตนา\n",
      "\n",
      "ขั้นตอนการวิเคราะห์:\n",
      "\n",
      "1. **ความเกี่ยวข้องของมาตรา 68**: มาตรา 68 ของประมวลกฎหมายอาญาระบุว่า \"ผู้ใดกระทำการโดยเจตนาเพื่อป้องกันตัวหรือป้องกันบุคคลอื่นจากการกระทำที่อาจทำให้เกิดอันตรายแก่ตนเองหรือบุคคลอื่น โดยไม่ใช้กำลังใดๆ ที่เกินไป\"\n",
      "\n",
      "2. **การวิเคราะห์เหตุการณ์**: นายแดงใช้มีดโจมตีนายดำหลังจากนายดำสดุดเท้าตัวเองแล้วล้มมา\n",
      "\n",
      "3. **การประเมินการใช้กำลังโดยเจตนา**: การกระทำของนายแดงในการใช้มีดเพื่อโจมตีนายดำไม่ได้เป็นการป้องกันตัวโดยชอบด้วยกฎหมาย แต่เป็นการละเมิดสิทธิของผู้อื่น และไม่มีการใช้กำลังโดยเจตนา\n",
      "\n",
      "4. **การประเมินความเกี่ยวข้องกับมาตรา 68**: เนื่องจากการกระทำของนายแดงไม่ได้เป็นการป้องกันตัวโดยชอบด้วยกฎหมาย แต่เป็นการละเมิดสิทธิของผู้อื่น และไม่มีการใช้กำลังโดยเจตนา จึงไม่ได้ตามมาตรา 68 ของประมวลกฎหมายอาญา\n",
      "\n",
      "คำแนะนำเพิ่มเติม: หากนายแดงต้องการอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ต้องมีการใช้กำลังโดยเจตนาและในสถานการณ์ที่จำเป็นในการป้องกันตัว เช่น การป้องกันการโจมตีของบุคคลอื่นโดยไม่ใช้กำลังใดๆ ที่เกินไป\n",
      "2024-10-13 21:04:44.606658\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question):   \n",
    "    # Query OpenThaiGPT\n",
    "    answer = query_openthaigpt(question, prompt[0])\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "answer = answer_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: นายดำเข้าแย่งมีดจากนายแดง และนายดำถูกแทงเสียชีวิต หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย นายแดงจะอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้หรือไม่ เพราะอะไร ให้ยกตัวอย่างคำพิพากษาศาลฎีกาหรือหมายเลขคดีแดงเรื่องนี้ด้วย\n",
      "Answer: การอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 กฎหมายแพทย์ไม่สามารถใช้ได้ในกรณีนี้ เนื่องจากข้อเท็จจริงที่ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย ซึ่งไม่ใช่การกระทำโดยเจตนา แต่เป็นการกระทำโดยประมาท\n",
      "\n",
      "การป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ต้องมีองค์ประกอบของการกระทำโดยเจตนาของผู้กระทำ และไม่มีการกระทำโดยเจตนาของตนเองในการป้องกันตัวในกรณีนี้ ไม่มีการกระทำที่เป็นการป้องกันตัวอย่างจริงจัง\n",
      "\n",
      "ดังนั้น นายแดงไม่สามารถอ้างว่าเป็นการป้องกันตัวโดยชอบด้วยกฎหมาย ตาม มาตรา 68 ได้ หากข้อเท็จจริงฟังได้ว่านายดำสดุดเท้าตัวเองแล้วล้มมาโดนมีดแทงจุดสำคัญเป็นเหตุให้ถึงแก่ความตาย\n",
      "\n",
      "หมายเลขคดีแดงเรื่องนี้ไม่ได้กล่าวไว้ในข้อมูลที่ให้มา\n"
     ]
    }
   ],
   "source": [
    "def answer_question(question2):   \n",
    "    # Query OpenThaiGPT\n",
    "    answer = query_openthaigpt(question2, prompt2[0])\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "answer = answer_question(question2)\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdai-llm-finalfix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
